{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "redo",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfVE8jslIZXfDijKl2+wjh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcoplacenti/protein_generation/blob/main/final/redo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrh7uny1abve"
      },
      "source": [
        "from google.colab import drive\n",
        "import torch, sys\n",
        "import pandas as pd\n",
        "import numpy as np  \n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "import torch.distributions as dist\n",
        "import random\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxw17ufMgNPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeca8da1-12fe-4042-a7c3-154550fde823"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS6Bs38vydfG",
        "outputId": "eb14c26d-8b04-4cba-9c43-8cf2d3ce0f5b"
      },
      "source": [
        "cd /content/drive/MyDrive/'Colab Notebooks'/project/"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0acy6-F7gq0D"
      },
      "source": [
        "def process_data(data, vocab, max_seq):\n",
        "    token_to_id, id_to_token = {}, {}\n",
        "    \n",
        "    token_to_id[\"<PAD>\"] = 0\n",
        "    id_to_token[0] = \"<PAD>\"\n",
        "\n",
        "    token_to_id[\"<EOS>\"] = 1\n",
        "    id_to_token[1] = \"<EOS>\"\n",
        "\n",
        "    token_to_id[\"<DUMMY>\"] = 2\n",
        "    id_to_token[2] = \"<DUMMY>\"\n",
        "\n",
        "    for i, token in enumerate(vocab):\n",
        "        cum_i = len(token_to_id.keys())\n",
        "        if token != \"<PAD>\" and token != \"<EOS>\" and token != \"<DUMMY>\":\n",
        "            token_to_id[token] = cum_i\n",
        "            id_to_token[cum_i] = token\n",
        "            cum_i += 1\n",
        "\n",
        "    seq = []\n",
        "    for record in data.values:\n",
        "        tags = record[:-1]\n",
        "        sequence = record[-1]\n",
        "        \n",
        "        encoded_record = [token_to_id[tag] for tag in tags]\n",
        "\n",
        "        for char in sequence:\n",
        "            encoded_record.append(token_to_id[char])\n",
        "        #encoded_record.append(token_to_id[\"<EOS>\"])\n",
        "        \n",
        "        if len(sequence) < max_seq:\n",
        "            for i in range(max_seq-len(sequence)):\n",
        "                encoded_record.append(token_to_id[\"<PAD>\"])\n",
        "\n",
        "        seq.append(encoded_record)\n",
        "\n",
        "    return np.array(seq), token_to_id, id_to_token"
      ],
      "execution_count": 433,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quzuw8D9g0RM"
      },
      "source": [
        "def get_data(dataset):\n",
        "    data = pd.read_csv(dataset)\n",
        "\n",
        "    data = data.replace(np.nan, '<DUMMY>', regex=True)\n",
        "    #data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
        "    #data.drop(\"Entry\", axis=1, inplace=True)\n",
        "    #print(data.columns)\n",
        "    max_length = int(data['Sequence'].str.len().max())\n",
        "    data = data[data[\"Sequence\"].map(len) <= max_length]\n",
        "    vocab = set()\n",
        "    for col in data.columns:\n",
        "        if col != \"Sequence\":\n",
        "            vocab.update(data[col])\n",
        "\n",
        "    seq_len = []\n",
        "    max_seq = 0\n",
        "    for seq in data[\"Sequence\"]:\n",
        "        seq = [s for s in seq]\n",
        "        seq_len.append(len(seq))\n",
        "        if len(seq) > max_seq:\n",
        "            max_seq = len(seq)\n",
        "        vocab.update(seq)\n",
        "\n",
        "    vocab.update([\"<PAD>\"])\n",
        "    vocab.update([\"<EOS>\"])\n",
        "\n",
        "    return data, vocab, max_seq"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWooCQBjgyw5"
      },
      "source": [
        "class ProGen(nn.Module):\n",
        "  \n",
        "  def __init__(self, hid_size, vocab_size, n_head, n_layers, pf_size, max_len, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "    self.hid_size = hid_size\n",
        "    self.pf_size = pf_size\n",
        "    self.max_len = max_len\n",
        "    self.embedding = nn.Embedding(vocab_size, hid_size)\n",
        "    self.position_enc = nn.Embedding(self.max_len, self.hid_size)\n",
        "    self.position_enc.weight.data = self.position_encoding_init(self.max_len, self.hid_size)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.hid_size])).to(device)\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(self.hid_size)\n",
        "    self.decoder_layer = nn.TransformerDecoderLayer(d_model=hid_size, nhead = n_head, dim_feedforward=self.pf_size)\n",
        "    self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=n_layers, norm=self.layer_norm)\n",
        "    self.fc = nn.Linear(hid_size, vocab_size)\n",
        "    self._init_weights()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    sent_len, batch_size = x.shape[0], x.shape[1]\n",
        "    memory_mask = self.generate_complete_mask(sent_len)\n",
        "    tgt_mask = self.generate_triangular_mask(sent_len)\n",
        "    memory = torch.zeros(1, batch_size, self.hid_size, device=self.device)\n",
        "\n",
        "    temp = x\n",
        "    temp = self.embedding(temp)\n",
        "\n",
        "    pos = torch.arange(0,sent_len).unsqueeze(1).repeat(1,batch_size).to(self.device)\n",
        "    temp_pos_emb = self.position_enc(pos)\n",
        "\n",
        "    temp = temp * self.scale + temp_pos_emb\n",
        "    temp = self.decoder(temp, memory, tgt_mask=tgt_mask)\n",
        "    temp = self.fc(temp)\n",
        "    return temp\n",
        "\n",
        "  def _init_weights(self):\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "  def append_decoder_layer(self):\n",
        "    appended_mod = nn.TransformerDecoderLayer(d_model=hid_size, nhead = n_head).to(self.device)\n",
        "    for p in appended_mod.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "    model.decoder.layers.append(appended_mod)\n",
        "    model.decoder.num_layers += 1\n",
        "\n",
        "  def generate_triangular_mask(self, size):\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n",
        "        return\n",
        "        \n",
        "  def generate_complete_mask(self, size):\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = torch.empty(size, size).to(device)\n",
        "        mask.fill_(float('-inf'))\n",
        "        return mask\n",
        "\n",
        "  def generate_sequence(self, src):\n",
        "    #src = [sent_len]\n",
        "    src = src.unsqueeze(1)\n",
        "    #src = [sent_len, 1]\n",
        "    generate_step = 0\n",
        "    while generate_step < 120:\n",
        "      out = self.forward(src)\n",
        "      #out = [sent_len + 1, 1, vocab_size]\n",
        "      out = torch.argmax(out[-1, :], dim=1) # [1]\n",
        "      out = out.unsqueeze(0) #[1,1]\n",
        "      src = torch.cat((src, out), dim=0)\n",
        "      generate_step += 1\n",
        "    src = src.squeeze(1)\n",
        "    return src\n",
        "  \n",
        "  def position_encoding_init(self, n_position, d_pos_vec):\n",
        "    ''' Init the sinusoid position encoding table '''\n",
        "\n",
        "    # keep dim 0 for padding token position encoding zero vector\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2*i/d_pos_vec) for i in range(d_pos_vec)]\n",
        "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
        "\n",
        "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
        "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
        "    temp = torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
        "    temp = temp.to(self.device)\n",
        "    return temp"
      ],
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27AlSYY3hptC"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1SpvL3Kkize"
      },
      "source": [
        "file = \"dataset.csv\"\n",
        "\n",
        "data, vocab, max_seq = get_data(file)\n",
        "seq, token_to_id, id_to_token = process_data(data, vocab, max_seq)\n",
        "tags_end=1#data.columns.get_loc(\"Sequence\")\n",
        "aa_read=0"
      ],
      "execution_count": 455,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4eX1Mqc-RDF",
        "outputId": "ee2bfb9b-86dd-49b4-d01b-bba875bd9de5"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "hid_size = 32\n",
        "pf_size = 16\n",
        "max_len = 120\n",
        "n_head = 4\n",
        "n_layer= 8\n",
        "model = ProGen(hid_size, vocab_size, n_head, n_layer, pf_size, max_len, device).to(device)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 161,122 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9i4nwfkJu-8"
      },
      "source": [
        "max_len=seq.shape[1] \n",
        "\n",
        "# Preprocess strings into tensors of char ascii indexes\n",
        "inputs  = torch.zeros((len(seq), max_len)).to(torch.long).to(device)\n",
        "targets = torch.zeros((len(seq), max_len)).to(torch.long).to(device)\n",
        "\n",
        "for i, word in enumerate(seq):\n",
        "    inputs[i,  0:len(word)]   = torch.from_numpy(word)\n",
        "    targets[i, tags_end+aa_read:] = torch.from_numpy(word[tags_end+aa_read:])\n",
        "    targets[i, len(word)-1]   = 1  # <EOS> token\n",
        "\n",
        "# Split into train and test dataset\n",
        "combined = torch.stack([inputs, targets], dim=1)\n",
        "train_size = int(0.8 * len(combined))\n",
        "test_size = len(combined) - train_size\n",
        "train_ds, test_ds = torch.utils.data.random_split(combined, [train_size, test_size])\n",
        "\n",
        "train_x, train_y = combined[train_ds.indices][:, 0, :], combined[train_ds.indices][:, 1, :]\n",
        "test_x, test_y   = combined[test_ds.indices][:, 0, :],  combined[test_ds.indices][:, 1, :]"
      ],
      "execution_count": 467,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIKbWpBHlTju",
        "outputId": "12669ca4-4f1f-47be-a9a0-cb9442015b73"
      },
      "source": [
        "stats = { 'epoch':[], 'train_loss': [], 'train_perplexity': [], 'test_loss': [], 'test_perplexity': [] }\n",
        "batch_size, epochs, clip = 16, 50, 500\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.98), eps=1e-9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(epochs):\n",
        "  model.train()\n",
        "  idxs = torch.randint(size=(batch_size,), low=0, high=len(train_x))\n",
        "  src = train_x[idxs]\n",
        "  trg = train_y[idxs]\n",
        "  output = model(src)\n",
        "  optimizer.zero_grad()\n",
        "  loss = criterion(output.view(-1, output.size(-1)), trg.view(-1))\n",
        "  nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  stats['train_loss'].append(loss.item())\n",
        "  stats['epoch'].append(i)\n",
        "  print(i, loss.item())"
      ],
      "execution_count": 468,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 7.063000679016113\n",
            "1 6.5903449058532715\n",
            "2 6.242940902709961\n",
            "3 5.607173442840576\n",
            "4 5.2226786613464355\n",
            "5 4.7251362800598145\n",
            "6 4.290638446807861\n",
            "7 3.961808204650879\n",
            "8 3.6334004402160645\n",
            "9 3.3569600582122803\n",
            "10 3.1745893955230713\n",
            "11 3.0178823471069336\n",
            "12 2.8997130393981934\n",
            "13 2.8201217651367188\n",
            "14 2.721745014190674\n",
            "15 2.6656618118286133\n",
            "16 2.7201120853424072\n",
            "17 2.5644783973693848\n",
            "18 2.512864589691162\n",
            "19 2.5231502056121826\n",
            "20 2.5379514694213867\n",
            "21 2.543693780899048\n",
            "22 2.495420217514038\n",
            "23 2.4478678703308105\n",
            "24 2.5313730239868164\n",
            "25 2.445011615753174\n",
            "26 2.408094882965088\n",
            "27 2.431363821029663\n",
            "28 2.44454026222229\n",
            "29 2.483577251434326\n",
            "30 2.4214274883270264\n",
            "31 2.4991891384124756\n",
            "32 2.408017635345459\n",
            "33 2.422708749771118\n",
            "34 2.431666851043701\n",
            "35 2.42036771774292\n",
            "36 2.4477670192718506\n",
            "37 2.4012019634246826\n",
            "38 2.4095044136047363\n",
            "39 2.4028615951538086\n",
            "40 2.4413394927978516\n",
            "41 2.4790995121002197\n",
            "42 2.5486178398132324\n",
            "43 2.3796942234039307\n",
            "44 2.424682378768921\n",
            "45 2.450481414794922\n",
            "46 2.4333810806274414\n",
            "47 2.4382569789886475\n",
            "48 2.3702611923217773\n",
            "49 2.426450252532959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "mcBrhx5dQr4b",
        "outputId": "bacd5d64-87c5-416c-d277-396acd2c7f98"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(stats['epoch'], stats['train_loss'])\n",
        "plt.show()"
      ],
      "execution_count": 470,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfgklEQVR4nO3deXSc9X3v8fd3No32fbEl27Ll3QYbWxi8kIBtjElcSEsgpE2gIYST3iykaZum7em9Te9pz03Pzdbl3MYJCQQSEpYGCAkGs282RgbbeJHBu2Vrt619G+l3/9CYOuBFtjV6Zvm8ztEZzWiQPj88+syj3/N7nsecc4iISPzyeR1ARETOTkUtIhLnVNQiInFORS0iEudU1CIicS4Qi29aVFTkKisrY/GtRUSS0ubNm1ucc8Wn+1pMirqyspKamppYfGsRkaRkZgfP9DVNfYiIxDkVtYhInFNRi4jEORW1iEicU1GLiMQ5FbWISJxTUYuIxLm4KeregUHWvryX1/a0eB1FRCSunLOozWyGmW055aPdzL422kGCfh9rX97Pg5sOjfa3FhFJaOc8MtE5txuYD2BmfuAI8OvRDuL3GdfOLuE3W+vpiwySFvCP9o8QEUlI5zv1sQLY65w746GOF2PV7DI6+yJs2Nsai28vIpKQzreobwUePN0XzOwuM6sxs5rm5uYLCrO4qpDMkJ9ndjZe0H8vIpKMRlzUZhYCbgAePt3XnXNrnXPVzrnq4uLTngDqnMJBP1fPKGH9zkaGhnQtRxEROL8t6uuBt5xzMd3cXTWnlOaOPrbUnYjljxERSRjnU9Sf5gzTHqPp6hklBHzGek1/iIgAIyxqM8sErgX+K7ZxIDc9yJVTCnlmR0Osf5SISEIYUVE757qcc4XOubZYB4Lh6Y+9zV3saeocix8nIhLX4ubIxFOtnFUKoOkPERHitKjH56VzaUUuz+zU9IeISFwWNcCq2aW8fegETe29XkcREfFU/Bb1nDIA1u/S9IeIpLa4LeppJVlMKszgmR0qahFJbXFb1GbGqtmlvL63hY7eAa/jiIh4Jm6LGoanPwYGHS+9e2HnDhERSQZxXdQLJuZTmBnS9IeIpLS4Lmq/z1g5q5QXapvojwx5HUdExBNxXdQwfJRiR1+Ejft0jmoRSU1xX9RLpxaREfLztM79ISIpKu6LOhz0s3xmCU9uq6enf9DrOCIiYy7uixrgs1dOoq1ngMe3HPE6iojImEuIol40uYCZZdnc+/oBnNOVX0QktSREUZsZty+ppLahg037j3kdR0RkTCVEUQN8Yn45uelBfrYhJhdAFxGJWwlT1OkhP5+6fALrdjRQ39bjdRwRkTGTMEUN8JkrJjHkHL9445DXUURExkxCFfXEwgxWzCzhwU2H6ItoqZ6IpIaEKmqA25dU0tLZz2+31XsdRURkTCRcUS+tKmJKcSb3aaeiiKSIhCtqn8+4fXElWw+fYMvhE17HERGJuYQraoCbFlaQlRbgvtcPeB1FRCTmErKos9IC3LSgnN9uq6e5o8/rOCIiMZWQRQ1w25JK+geH+OUmLdUTkeSWsEVdVZzFVdOK+MWmQzr/h4gktYQtaoCPXzKO+rZe9rd0eR1FRCRmErqoqyvzAag5cNzjJCIisZPQRT2lKIu8jCA1B3VGPRFJXgld1D6fUT0pn5qD2qIWkeSV0EUNsHBSAfuau2jt1DI9EUlOCV/UJ+epN2urWkSSVMIX9SXluYT8Pk1/iEjSGlFRm1memT1iZrVmtsvMFsc62EiFg34uqcil5oB2KIpIchrpFvUPgHXOuZnAPGBX7CKdv+rKfN450kbvgM5RLSLJ55xFbWa5wEeAewCcc/3Oubg6bV31pAIGBh3b6tq8jiIiMupGskU9GWgGfmpmb5vZj80s84NPMrO7zKzGzGqam5tHPejZLJwUPfBF66lFJAmNpKgDwALg/znnLgO6gG9+8EnOubXOuWrnXHVxcfEoxzy7gswQVcWZOkJRRJLSSIq6Dqhzzr0Rvf8Iw8UdV6onFbD54HGGhnSCJhFJLucsaudcA3DYzGZEH1oB7IxpqguwsDKftp4B9jR3eh1FRGRUBUb4vK8APzezELAP+FzsIl2YyysLgOETNE0vzfY4jYjI6BnR8jzn3Jbo/POlzrlPOOfibjK4sjCDwsyQdiiKSNJJ+CMTTzIzFk7K1w5FEUk6SVPUMDz9cehYN03tvV5HEREZNUlV1AtPXkhA5/0QkSSSVEU9d3wuaQGfpj9EJKkkVVGHAj7mTchjs3YoikgSSaqiBqielM/2o+1090e8jiIiMiqSrqgvryxgcMix5XBcnTdKROSCJV1RL5ioK5OLSHJJuqLOzQgyvTRLKz9EJGkkXVEDVFcW8PbB4wzqBE0ikgSSsqgvr8ynoy/C1jrNU4tI4kvKol4xq5S0gI9HN9d5HUVE5KIlZVHnhIN87JJxPLHlKD39uo6iiCS2pCxqgJurK+joi/D0jgavo4iIXJSkLeorJxcyoSCdh2oOex1FROSiJG1R+3zGzQsn8PreVg4f6/Y6jojIBUvaogb45MIKzOBhbVWLSAJL6qIen5fOVdOKeWRzndZUi0jCSuqiBriluoKjbb28tqfF6ygiIhck6Yv62tml5GUEtVNRRBJW0hd1WsDPJ+aX88yORk5093sdR0TkvCV9UQPcUj2B/sEhHt9y1OsoIiLnLSWKevb4HOaW5/CrNzX9ISKJJyWKGoa3qnfWt7P9SJvXUUREzkvKFPWN88oJBXxaUy0iCSdlijo3I8jqOWU8tuUovQM6UZOIJI6UKWoYnv5o6xnguV1NXkcRERmxlCrqxVWFFGSGWL9TZ9QTkcSRUkXt9xlXzyjmxXebdUi5iCSMlCpqgBUzSznRPcDbh3TxWxFJDClX1FdNLyLgM56r1Ty1iCSGlCvqnHCQRZMLeF47FEUkQaRcUQMsn1nC7sYOXVBARBLCiIrazA6Y2TtmtsXMamIdKtaWzywB4IXd2qoWkfh3PlvU1zjn5jvnqmOWZoxMKc5iclGm1lOLSEJIyakPGN6q3rCvle7+iNdRRETOaqRF7YBnzGyzmd11uieY2V1mVmNmNc3NzaOXMEZWzCyhPzLEa3tavY4iInJWIy3qZc65BcD1wJfM7CMffIJzbq1zrto5V11cXDyqIWOhurKA7LQAz9c2eh1FROSsRlTUzrkj0dsm4NfAoliGGguhgI+PTC/muV1NOKejFEUkfp2zqM0s08yyT34OrAK2xzrYWLhmZglNHX3sONrudRQRkTMayRZ1KfCqmW0FNgG/dc6ti22ssXH1jGLM0OoPEYlrgXM9wTm3D5g3BlnGXFFWGvMn5PF8bSN3r5zmdRwRkdNK2eV5J62YWcLWujaaOnq9jiIiclopX9TLZ5YC8OLu+F9SKCKpKeWLeta4bMblhnWSJhGJWylf1GbG8pklvPJeM30RXUtRROJPyhc1DB9O3tU/yKb9x7yOIiLyISpqYElVEWkBH8/u1FGKIhJ/VNRAesjP1TOKeWp7g66lKCJxR0UdtebS8TR19PHmAU1/iEh8UVFHrZhVQjjo48ltR72OIiLye1TUURmhACtmlvLUOw1EBoe8jiMi8j4V9SnWXDqO1q5+Nu7T9IeIxA8V9SmumVlCZsiv6Q8RiSsq6lOEg35Wzi5l3Y4GBjT9ISJxQkX9AWsuHc+J7gFe29PidRQREUBF/SEfmV5EdjjAk9vqvY4iIgKoqD8kLeBn1ewynt7RoHN/iEhcUFGfxpp54+jojfDKu5r+EBHvqahPY2lVEbnpQa3+EJG4oKI+jVDAx+o5Zazf2UjvgKY/RMRbKuozWDNvHF39g7y4WxcUEBFvqajPYPGUQgozQ/xGqz9ExGMq6jMI+H2snlvG87ua6O6PeB1HRFKYivos1lw6np6BQZ6v1fSHiHhHRX0WiyYXUJydxm+2avWHiHhHRX0Wfp/xh5eV8+yuJo6c6PE6joikKBX1Ody+pBKAe1/b720QEUlZKupzKM9L52OXjOOXmw7T0TvgdRwRSUEq6hH4wlWT6eiL8Ks3D3sdRURSkIp6BC6tyGPR5AJ++toBXaZLRMacinqE7lw2mSMnenhqe4PXUUQkxaioR2jlrFImF2Xy41f24ZzzOo6IpBAV9Qj5fMYdyyazta6NmoPHvY4jIilERX0ePrmggryMID96eZ/XUUQkhYy4qM3Mb2Zvm9mTsQwUz9JDfj5zxSTW72pkf0uX13FEJEWczxb13cCuWAVJFLctmUTQ5+Mnr+oAGBEZGyMqajOrAD4O/Di2ceJfSXaYG+eP5+HNhznR3e91HBFJASPdov4+8A3gjIuIzewuM6sxs5rm5uZRCRev7rxqCr0DQ/z8jUNeRxGRFHDOojazNUCTc27z2Z7nnFvrnKt2zlUXFxePWsB4NKMsm49ML+be1w/oSuUiEnMj2aJeCtxgZgeAXwLLzeyBmKZKAHcum0xzRx9PbNEpUEUkts5Z1M65v3HOVTjnKoFbgeedc5+JebI4d9W0ImaWZXPPq/t1AIyIxJTWUV8gM+PzyyZT29DBK++1eB1HRJLYeRW1c+5F59yaWIVJNDfMH09xdho/ekUHwIhI7GiL+iKkBfz86ZJKXnmvhdqGdq/jiEiSUlFfpD+5YiLpQT8/fkUHwIhIbKioL1JeRoibqyt4fMsRmtp7vY4jIklIRT0K7lg6mciQ474NB7yOIiJJSEU9CiqLMlk1u5QHNh6iuz/idRwRSTIq6lHyhaum0NYzwCOb67yOIiJJRkU9ShZOymf+hDzueXU/g0M6AEZERo+KepSYGV+4agoHW7tZv7PR6zgikkRU1KPoujmlVOSn82MdACMio0hFPYoCfh93LJ1MzcHj1Bw45nUcEUkSKupRduuiCRRnp/Ev63brZE0iMipU1KMsIxTgqyumsenAMV7cndwXUBCRsaGijoFbL5/ApMIMvr2uliGtABGRi6SijoGg38dfrJpBbUMHT2zVhQVE5OKoqGNkzSXjmDM+h++s301/5IyXmhQROScVdYz4fMY3Vs/k8LEeHtyki+CKyIVTUcfQR6YVceWUAv7t+ffo6tM5QETkwqioY8jM+OvVM2np7OeeV3W+ahG5MCrqGLtsYj7XzSll7cv7aO3s8zqOiCQgFfUY+KvrZtDdH+E/XtjrdRQRSUAq6jEwtSSbTy6s4IGNB6k73u11HBFJMCrqMfK1ldPx+eDrv9qq5Xoicl5U1GNkfF46377pUjYdOMY/PrnD6zgikkACXgdIJTfOL2dnfTs/fGkfc8bn8ulFE72OJCIJQFvUY+wb183ko9OL+Z+Pb9epUEVkRFTUY8zvM/711ssoz0vniw+8RX1bj9eRRCTOqag9kJsR5Ee3VdPTH+GL92+md2DQ60giEsdU1B6ZVprN9z41n611bfztr9/RRQZE5IxU1B5aNaeMP185nf966wg/0nUWReQMtOrDY19ZPpV3Gzv459/VkpcR4pbqCV5HEpE4o6L2mM9nfPdT82jvHeCbj24jKy3Axy4Z53UsEYkjmvqIA2kBPz/87EIWTMzn7l++zYu7m7yOJCJxREUdJzJCAX7yucuZXprNFx/YzKb9WmMtIsPOWdRmFjazTWa21cx2mNm3xiJYKsoJB/nZHYsoz0vn8/e+yfYjbV5HEpE4MJIt6j5guXNuHjAfWG1mV8Y2VuoqzErjgTuvICc9yG0/2cSepg6vI4mIx85Z1G5YZ/RuMPqhRb8xNC43nZ/feQV+n3Hr2o2aBhFJcSOaozYzv5ltAZqA9c65N07znLvMrMbMapqbm0c7Z8qpLMrkl3ddSU44yB//aCP3bzigg2JEUtSIito5N+icmw9UAIvMbO5pnrPWOVftnKsuLi4e7Zwpqao4i8e+vJSPTi/m7x/fwV8/uk2Hm4ukoPNa9eGcOwG8AKyOTRz5oJzw8HlBvrp8Kg/V1PGptRtpaOv1OpaIjKGRrPooNrO86OfpwLVAbayDyX/z+Yyvr5rBDz+7kD2NHaz5t1d5U6dIFUkZI9miHge8YGbbgDcZnqN+Mrax5HSum1PGY19aSnY4wKd+uIFvPrpNW9ciKcBisYOqurra1dTUjPr3lWHtvQP84Nn3uH/DQczgjmWT+eJHq8hND3odTUQukJltds5Vn/ZrKurEdfhYN99d/y6PbTlCTjjIl6+ZymcXTyIc9HsdTUTO09mKWoeQJ7AJBRl871PzefIry5g/IY9/+t0uVnznJZ7b1eh1NBEZRSrqJDBnfC733bGIX9x5BdnhAJ+/r4avP7SFtu4Br6OJyChQUSeRJVOLeOLLy/jq8qk8vuUoq77/Es/XautaJNGpqJNMKODj66tm8PiXlpKfEeKOe7V1LZLoVNRJam557u9tXV/7vZe4f+NB2nrOXdidfREe2VzH/RsP6rB1kTigVR8pYPuR4QvobqtrIxTwsXpOGTdXV7C0qgifzwAYHHJs2NvKo2/VsW57Az3RQ9W/fM1U/vK6GV7GF0kJZ1v1oUtxpYC55bk8/qWl7DjazkM1h3l8y1Ge2HqU8blhblpYQWTI8djbR6hv6yU7HOATl5Vz04JyHtlcx7+/sIfMtAB/dnWV18MQSVkq6hRhZswtz2VueS5/+7FZrN/ZyMPRIvaZ8dHpxfzdx2exclbp++uwL5uYT1f/IN9eV0tWmp/PLq70dhAiKUpFnYLCQT9/MG88fzBvPE0dvfjNKMxK+9Dz/D7ju7fMo6c/wt8/voOMUICbFlZ4kFgktWlnYooryQ6ftqRPCvp9/PsfL2Dp1EL+6pGtrNteP4bpRARU1DIC4aCftZ+tZv6EPL7yoK6SLjLWVNQyIplpAX76uUVMK8nmCz+r4c8e2MxT79SP2oUMnHP0DgxyvKuf+rYejnX1j8r3lcQVGRzitT0tdPdHvI7iOc1Ry4jlpgd54M4r+Nfn3uPJbfU8tb2B7LQA180t48b541k8pZCA34dzju7+Qdp6BjjRPcCJnn6aO/poau+jqaOXxvY+Gtt7ae7o40TPAD39g/RGBjl1pagZLJyYz/WXjGP13DLK89LPO29H7wAPbjpESXaYG+aNf38posS33oFBHn2rjh++tI9Dx7pZUlXIvZ9bRCiQutuVWkctFyQyOMTGfcd4fMsR1m1voKMvQl5GkIDPR1tPPwODp39dhYM+SnPClGaHKclJIy8jSEYoQDjgIxzykx4c/mhs7+Op7fXUNgxfhX3ehDw+NreM6+eOY2JhxlmzDQ45Hqo5zHee2U1L5/CW+fwJeXzrhjnMm5A3uv8jRlHvwCD1bb1MLsr0OoonOnoH+Pkbh7jn1f00d/QxryKXxVVF/OdLe/mjBeV85+Z5mCXvm61Ocyox1TswyIu7m3i+tomA30duepC89CB5GUFy00Pkpgcpzg5RkhMmOy1wXr9s+1u6eGp7PU+908A7R9oAmD0uh+vnlrF6bhnTSrN/7/mv7Wnhfz+5k9qGDqon5fN3H5/FvuYu/s+6Wpo7+riluoK/um4mxdln3oF6sXoHBnnr4HE27GulKCuNP1xQTk74zOcKjwwO8fDmOn7w7Hs0tPdy7exS/uGGORf0V0SiGRxy7DzazlPb67l/40E6eiMsm1rE/7i6isVVhZgZP3j2Pb737LvcvWIaf37tdK8jx4yKWpLC4WPdPLW9nnXbG3jr0AkAqoozWT23jCsmF/KzDQd4dlcTFfnpfPP6mXz8knHvvyl09A7w78/v4Sev7Scc8HP3ymncvqSSoP/i/5w+WTav7mnh9b0tbNp/jL7IED6DIQcZIT9/tKCc2xZXMv2UN5ahIcfvttfznWfeZX9LF/Mn5LF0aiH3vLofnxl/vnI6f7p0dDKei3OOrv5BstJiOxs6NOR4t6mDDXtbeX1vK2/sa6W9N4IZrJ5Txhc/WvWhv3qcc3zjkW08vLmOf/nkpdxSPeG8f25L5/DUW2lOGgWZobjcMldRS9JpbO/l6R0NrNvewBv7jzE45MhKC/Cla6byuaWVZ7x4wt7mTv7xNzt56d1mirLSWFxVyJVTCrhySiFTijI/9AvsnKOpo489TZ3sb+miqaOPls4+Wjv7aOnsf78ATh5yP6M0m6VTi1g2rZBFkwvZ19zJzzYc5ImtR+mPDLF4SiG3L5lEWtDP/316NzuOtjO9NIu/XDWDa2eXYmYcPtbNPzyxg+dqm5hZls0//9ElLJiYf9rx9EeGaOsZoL13gI7eCO090dveAXwG+RkhCjJD5EVvc9ODDAwO8W5jB7vq29l5tJ2d9e3U1nfQ0Rfh0opcrptTxnVzSplakn3an3k+nHMcbO3m9b2tvLa3hY17W2mN7iieWJDB4imFLJlayOIphZTkhM/4fQYGh7jj3jfZsLeVn37ucq6aVnza5/UODFLb0MHuhvbo7fBH6yk7p9MCPsrz0hmXF2Z8bjqlOWEGozuz+yJD798ORIZYOrWIm6sryAid+w2ssy/Ce40dXHaGf6tzUVFLUjvW1c+m/cdYOCl/RFMazjle3N3MY1uOsGFvK00dfQCUZKdx5ZRCqoqzOHisi71Nnext7qKz779XHZhBQUaIwqwQRVlp73/Mm5DL4qpCSrJPXzbHuvp5qOYw9284yJETPQBU5Kfz9Wunc+P8cvy+D79BPL2jkW/9ZgcN7b3ctKCCkuw0GqM7ZE/umD1+nmdFNANjeEsfIDPkZ9a4HGaNy6EgM8RL7zaz5fDwXytTijNZNbuMlbNKCAf9dPZF6OiN0Nk3QGdvhI6+CH4zMkJ+MkIBMkJ+0kN+MtMCHDnew2t7Wnh9b+v74y3NSWNJVRFLqgpZXFVIRf7Z9zV8UEfvADf/5wbqjvfw8BcXM2tczvv/b5+vbeLZnY28/F4z3f3Db5rpQT/TS7OYUZbNjLIcxuWGaWrv5WhbL0dO9HA0+tHU0UfAZ4QDftKCPtKit0NDjgOt3eRnBLltcSW3L6mkIDP0oX+nN/Yf4+GaOn73Tj3pIT8b/2bFBe34VFGLnIFzw7+MG/e1snFf6/vFXZYTZmpJFlXFmdHbLCYXZ1KclUbgIqYiBoccL9Q20d47wJpLx5/zF7qzL8L317/LT18/gDH8ZlKcE6Y0O42SnDRKssPkZ4bICQfIDgfICQfJDgfJDgcYco7jXQMc7+7neHc/x7r63y/2WWXZzB6fw4T8jA+thmlo62X9zgae3tHIxn2tRIYurCNy04MsnlLI0qmFLK4qoqr4w3+xnK/6th7+8D9eB+C2JZN4obaJzQePM+SG3whWzirlqmlFzCzLYWLBh8d2Os65M+aqOXCM/3xpH8/uaiQc9HFL9QTuXDaFgN94dHMdj7xVx8HWbrLSAvzBvHF8cuEEFkzMu6BxqqhFRsg5R19kKO6uO9k7MEjI7xvzJYZt3QNs2NcCGDnhAFnhANnhIFlpAbLSAjgcXX2D9PQP0tUfobt/kO7+CPkZIWaNy/nQXwqjYefRdm754QY6+yLMHpfDytmlXDurlLnlOTGbe97T1MHal/fx67ePMDjkcIBzsHhKITdXV7B6btmIpkfORkUtIkml7ng3wHlPn1ysxvZefr7xIGbGTQsqzrlU9HzoNKciklTGuqBPKs0J8/VVY39+9tQ91EdEJEGoqEVE4pyKWkQkzqmoRUTinIpaRCTOqahFROKcilpEJM6pqEVE4lxMjkw0s2bg4AX+50VAyyjGSRQad2rRuFPLSMY9yTl32tMCxqSoL4aZ1ZzpMMpkpnGnFo07tVzsuDX1ISIS51TUIiJxLh6Leq3XATyicacWjTu1XNS4426OWkREfl88blGLiMgpVNQiInEuborazFab2W4z22Nm3/Q6TyyZ2U/MrMnMtp/yWIGZrTez96K3F3Yp4zhlZhPM7AUz22lmO8zs7ujjST1uADMLm9kmM9saHfu3oo9PNrM3oq/5X5lZ6FzfK9GYmd/M3jazJ6P3k37MAGZ2wMzeMbMtZlYTfeyCX+txUdRm5gf+A7gemA182sxme5sqpu4FVn/gsW8CzznnpgHPRe8nkwjwF8652cCVwJei/8bJPm6APmC5c24eMB9YbWZXAt8GvuecmwocBz7vYcZYuRvYdcr9VBjzSdc45+afsn76gl/rcVHUwCJgj3Nun3OuH/glcKPHmWLGOfcycOwDD98I3Bf9/D7gE2MaKsacc/XOubein3cw/MtbTpKPG8AN64zeDUY/HLAceCT6eNKN3cwqgI8DP47eN5J8zOdwwa/1eCnqcuDwKffroo+lklLnXH308wag1MswsWRmlcBlwBukyLijUwBbgCZgPbAXOOGci0Sfkoyv+e8D3wCGovcLSf4xn+SAZ8xss5ndFX3sgl/rurhtHHLOOTNLynWTZpYFPAp8zTnXPryRNSyZx+2cGwTmm1ke8GtgpseRYsrM1gBNzrnNZna113k8sMw5d8TMSoD1ZlZ76hfP97UeL1vUR4AJp9yviD6WShrNbBxA9LbJ4zyjzsyCDJf0z51z/xV9OOnHfSrn3AngBWAxkGdmJzeWku01vxS4wcwOMDyVuRz4Ack95vc5545Eb5sYfmNexEW81uOlqN8EpkX3CIeAW4EnPM401p4Abo9+fjvwuIdZRl10fvIeYJdz7runfCmpxw1gZsXRLWnMLB24luE5+heAT0afllRjd879jXOuwjlXyfDv8/POuT8hicd8kpllmln2yc+BVcB2LuK1HjdHJprZxxie0/IDP3HO/ZPHkWLGzB4Ermb41IeNwP8CHgMeAiYyfIrYW5xzH9zhmLDMbBnwCvAO/z1n+bcMz1Mn7bgBzOxShnce+RneOHrIOfePZjaF4a3NAuBt4DPOuT7vksZGdOrjL51za1JhzNEx/jp6NwD8wjn3T2ZWyAW+1uOmqEVE5PTiZepDRETOQEUtIhLnVNQiInFORS0iEudU1CIicU5FLSIS51TUIiJx7v8DSoPMKNN8NsAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-clZE_xVnPG",
        "outputId": "444db71a-2b76-4553-dfae-95f285f564fe"
      },
      "source": [
        "test_x[1,50:]"
      ],
      "execution_count": 475,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  2,   2,   2,   2,   2,   2,   2,   2,   2,   2, 492, 426, 426, 832,\n",
              "        607, 671, 144, 174, 933, 426, 607, 426,  84, 607, 426, 607,  45, 426,\n",
              "        607, 426,  45, 607, 164, 933, 607,  84,  40,  84, 806, 144, 144, 671,\n",
              "        144, 551, 956, 144, 164, 551,  40,  84, 144, 144, 551, 551, 607,  84,\n",
              "        671, 806, 806, 933,  84, 607, 100,  28, 806, 832, 956, 607, 832, 933,\n",
              "        100, 475, 100, 806, 164, 671, 100, 901,  84, 475, 551,  40, 426, 426,\n",
              "         84, 551, 607, 607, 572, 164, 164,  40, 956, 956, 100,  40, 475, 100,\n",
              "        901, 100, 806,  40,  40, 901, 806, 492, 174,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 475
        }
      ]
    }
  ]
}