{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mini_pro_gen_final",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/wIjThg4FE7qO987eCGde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcoplacenti/protein_generation/blob/main/final/mini_pro_gen_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6v97BiiDCI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import torch, sys\n",
        "import pandas as pd\n",
        "import numpy as np  \n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.distributions as dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPCxY0P0iJuJ"
      },
      "source": [
        "cd /content/drive/MyDrive/'Colab Notebooks'/project/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ys2IdwCs1Vk"
      },
      "source": [
        "from model import Transformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvFReLWIs1yP"
      },
      "source": [
        "def sample_categorical(lnprobs, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample an element from a categorical distribution\n",
        "    :param lnprobs: Outcome log-probabilities\n",
        "    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
        "        0.0 returns the maximum probability element.\n",
        "    :return: The index of the sampled element.\n",
        "    \"\"\"\n",
        "\n",
        "    if temperature == 0.0:\n",
        "        return lnprobs.argmax()\n",
        "    p = F.softmax(lnprobs / temperature, dim=0)\n",
        "    return dist.Categorical(p).sample()\n",
        "\n",
        "def sample_sentence(model, query, max_len = 140, temperature=1):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    for _ in range(max_len - len(query)):\n",
        "        #print(_)\n",
        "        query_ = torch.zeros(max_len).to(torch.long)\n",
        "        query_[:len(query)] = query\n",
        "        #print(make_sequence_from_tokens(query_, id_to_token))\n",
        "        output, _     = model(query_.unsqueeze(0).to(device))\n",
        "        #print(output)\n",
        "        next_char_idx = sample_categorical(output[0, :, len(query) - 1], 0.5) #0.5\n",
        "        #print(next_char_idx)\n",
        "\n",
        "        query = query.tolist()\n",
        "        query.append(int(next_char_idx))\n",
        "        query = torch.from_numpy(np.array(query))\n",
        "        #print(make_sequence_from_tokens(query, id_to_token))\n",
        "        #print(query.shape)\n",
        "\n",
        "    \n",
        "    return query\n",
        "\n",
        "\n",
        "def process_data(data, vocab, max_seq):\n",
        "    token_to_id, id_to_token = {}, {}\n",
        "    \n",
        "    token_to_id[\"<PAD>\"] = 0\n",
        "    id_to_token[0] = \"<PAD>\"\n",
        "\n",
        "    token_to_id[\"<EOS>\"] = 1\n",
        "    id_to_token[1] = \"<EOS>\"\n",
        "\n",
        "    token_to_id[\"<DUMMY>\"] = 2\n",
        "    id_to_token[2] = \"<DUMMY>\"\n",
        "\n",
        "    for i, token in enumerate(vocab):\n",
        "        cum_i = len(token_to_id.keys())\n",
        "        if token != \"<PAD>\" and token != \"<EOS>\" and token != \"<DUMMY>\":\n",
        "            token_to_id[token] = cum_i\n",
        "            id_to_token[cum_i] = token\n",
        "            cum_i += 1\n",
        "\n",
        "    seq = []\n",
        "    for record in data.values:\n",
        "        tags = record[:-1]\n",
        "        sequence = record[-1]\n",
        "        \n",
        "        encoded_record = [token_to_id[tag] for tag in tags]\n",
        "\n",
        "        for char in sequence:\n",
        "            encoded_record.append(token_to_id[char])\n",
        "        encoded_record.append(token_to_id[\"<EOS>\"])\n",
        "        \n",
        "        if len(sequence) < max_seq:\n",
        "            for i in range(max_seq-len(sequence)):\n",
        "                encoded_record.append(token_to_id[\"<PAD>\"])\n",
        "\n",
        "        seq.append(encoded_record)\n",
        "\n",
        "    return np.array(seq), token_to_id, id_to_token\n",
        "\n",
        "def get_data(dataset):\n",
        "    data = pd.read_csv(dataset)\n",
        "\n",
        "    data = data.replace(np.nan, '<DUMMY>', regex=True)\n",
        "    #data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
        "    #data.drop(\"Entry\", axis=1, inplace=True)\n",
        "    #print(data.columns)\n",
        "    max_length = int(data['Sequence'].str.len().max())\n",
        "    data = data[data[\"Sequence\"].map(len) <= max_length]\n",
        "    vocab = set()\n",
        "    for col in data.columns:\n",
        "        if col != \"Sequence\":\n",
        "            vocab.update(data[col])\n",
        "\n",
        "    seq_len = []\n",
        "    max_seq = 0\n",
        "    for seq in data[\"Sequence\"]:\n",
        "        seq = [s for s in seq]\n",
        "        seq_len.append(len(seq))\n",
        "        if len(seq) > max_seq:\n",
        "            max_seq = len(seq)\n",
        "        vocab.update(seq)\n",
        "\n",
        "    vocab.update([\"<PAD>\"])\n",
        "    vocab.update([\"<EOS>\"])\n",
        "\n",
        "    return data, vocab, max_seq\n",
        "\n",
        "def make_sequence_from_tokens(ids, id_to_token):\n",
        "    sequence = map(lambda x: id_to_token[x], ids.tolist())\n",
        "    return \"\".join(list(sequence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DsIRsIWtoZe"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "learning_rate, batch_size, epochs = 1e-2, 8, 200\n",
        "\n",
        "file = \"dataset.csv\"\n",
        "\n",
        "data, vocab, max_seq = get_data(file)\n",
        "seq, token_to_id, id_to_token = process_data(data, vocab, max_seq)\n",
        "\n",
        "tags_end=data.columns.get_loc(\"Sequence\")\n",
        "aastart=15\n",
        "\n",
        "max_len= seq.shape[1]\n",
        "\n",
        "# Preprocess strings into tensors of char ascii indexes\n",
        "inputs  = torch.zeros(seq.shape).to(torch.long).to(device)\n",
        "targets = torch.zeros(seq.shape).to(torch.long).to(device)\n",
        "\n",
        "for i, tag in enumerate(seq):\n",
        "     #print(i,tag)\n",
        "     inputs[i,0:len(tag)] = torch.from_numpy(tag)\n",
        "     #print(i,inputs[i])\n",
        "     targets[i,tags_end+aastart:] = torch.from_numpy(tag[tags_end+aastart:])\n",
        "     #print(i,targets[i])\n",
        "     #targets[i, len(tag)-1]   = 1 \n",
        "     #if i>tags_end+aastart: break\n",
        "\n",
        "# Split into train and test dataset\n",
        "combined = torch.stack([inputs, targets], dim=1)\n",
        "train_size = int(0.8 * len(combined))\n",
        "test_size = len(combined) - train_size\n",
        "train_ds, test_ds = torch.utils.data.random_split(combined, [train_size, test_size])\n",
        "\n",
        "train_x, train_y = combined[train_ds.indices][:, 0, :], combined[train_ds.indices][:, 1, :]\n",
        "test_x, test_y   = combined[test_ds.indices][:, 0, :],  combined[test_ds.indices][:, 1, :]\n",
        "\n",
        "max_index = int(max(train_x.max(), test_x.max()))\n",
        "\n",
        "args = {\n",
        "    'emb_dim':        32,            # Embedding vector dimension\n",
        "    'n_att_heads':    4,             # Number of attention heads for each transformer block\n",
        "    'n_transformers': 8,             # Depth of the network (nr. of self-attention layers)\n",
        "    'seq_length':     max_len,       # Sequence length\n",
        "    'num_tokens':     max_index + 1, # Vocabulary size (highest index found in dataset)\n",
        "    'device':         device,        # Device: cuda/cpu\n",
        "    'wide':           False          # Narrow or wide self-attention\n",
        "}\n",
        "\n",
        "stats = { 'epoch':[], 'train_loss': [], 'train_perplexity': [], 'test_loss': [], 'test_perplexity': [] }\n",
        "model = Transformer(**args).to(device)\n",
        "opt   = torch.optim.Adam(lr=learning_rate, params=model.parameters())\n",
        "\n",
        "QUERY = train_x[2,0:tags_end+aastart]\n",
        "\n",
        "print(QUERY)\n",
        "\n",
        "for i in range(epochs):\n",
        "    model.train()\n",
        "    opt.zero_grad()\n",
        "    \n",
        "    # Sample a random batch of size `batch_size` from the train dataset\n",
        "    idxs = torch.randint(size=(batch_size,), low=0, high=len(train_x))\n",
        "    \n",
        "    output, (emb_mean, emb_max) = model(train_x[idxs])\n",
        "    loss = F.nll_loss(output, train_y[idxs], reduction='mean')\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "    perplexity_train = torch.exp(loss).item()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    \n",
        "    # Calculate perplexity on the test-set\n",
        "    model.eval()\n",
        "    output_test, _ = model(test_x)\n",
        "    loss_test = F.nll_loss(output_test, test_y, reduction='mean')\n",
        "    perplexity_test = torch.exp(loss_test).item()\n",
        "\n",
        "    # Update the stats and print something.\n",
        "    stats['train_loss'].append(loss.item())\n",
        "    stats['train_perplexity'].append(perplexity_train)\n",
        "    stats['test_loss'].append(loss_test.item())\n",
        "    stats['test_perplexity'].append(perplexity_test)\n",
        "    stats['epoch'].append(i)\n",
        "    \n",
        "    \n",
        "    sampled  = make_sequence_from_tokens(sample_sentence(model, QUERY,\n",
        "                                                         max_len = max_len,\n",
        "                                                         temperature = 0), id_to_token)\n",
        "\n",
        "    to_print = [\n",
        "        f\"EPOCH %03d\"        % i,\n",
        "        f\"trainLOSS %4.4f\"        % stats['train_loss'][-1],\n",
        "        f\"testLOSS %4.4f\"        % stats['test_loss'][-1],\n",
        "        f\"PERPLEXITY %4.4f\" % stats['test_perplexity'][-1],\n",
        "        f\"\\t%s\"      % sampled\n",
        "    ]\n",
        "    print(\" \".join(to_print))\n",
        "\n",
        "# Finally, save everyting:\n",
        "torch.save({\n",
        "    'state_dict':   model.state_dict(), \n",
        "    'stats':        stats,\n",
        "    'args':         args,\n",
        "    'train_x':      train_x,\n",
        "    'test_x':       test_x\n",
        "}, f\"words.model.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8J3zEGLUGKN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(stats['epoch'], stats['test_perplexity'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "250A9PacYL1J"
      },
      "source": [
        "plt.plot(stats['epoch'], stats['test_loss'])\n",
        "plt.plot(stats['epoch'], stats['train_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrk7kZSiY8yG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}